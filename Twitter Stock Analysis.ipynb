{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow v2\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "import torch\n",
    "\n",
    "# import keyword extraction\n",
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "# import data manipulation lib\n",
    "from scipy import spatial\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# import networkx \n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import greedy_modularity_communities, asyn_lpa_communities\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "import re\n",
    "twitter_username_re = re.compile(r'@([A-Za-z0-9_]+)')\n",
    "twitter_url_re = re.compile(r'https?:\\/\\/\\S*')\n",
    "twitter_hashtag_re = re.compile(r'#(\\w+)')\n",
    "twitter_long_chars_re = re.compile(r'[A-Za-z0-9_]{15,}')\n",
    "\n",
    "# process twitter\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Twitter Public Thread Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1628"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_to_analyze = set()\n",
    "with open('2_elon.json') as f:\n",
    "    obj = json.load(f)\n",
    "    for o in obj:\n",
    "        original_text = o['text']\n",
    "        text = re.sub(twitter_username_re, '', original_text)\n",
    "        text = re.sub(twitter_url_re, '', text)\n",
    "        text = re.sub(twitter_hashtag_re, '', text)\n",
    "        text = re.sub(twitter_long_chars_re, '', text)\n",
    "        text = text.replace('\\n', '. ').replace('&amp;', '').replace('amp;', '').replace('&gt;', '').replace('&lt;', '').strip()\n",
    "        text = text.lower()\n",
    "        if 'elonmusk-giveaway.org' in text:\n",
    "            continue\n",
    "        if 'www.elon-btc.org' in text:\n",
    "            continue\n",
    "        if 'www.elontrust.com' in text:\n",
    "            continue\n",
    "        if 'www.elon-btc' in text:\n",
    "            continue\n",
    "        if 'elonx .club' in text:\n",
    "            continue\n",
    "        if 'musktop' in text:\n",
    "            continue\n",
    "        if 'elonmusk-giveaway' in text:\n",
    "            continue\n",
    "        if text:\n",
    "            texts_to_analyze.add(text)\n",
    "        \n",
    "\n",
    "texts_to_analyze = list(texts_to_analyze)\n",
    "#for text in texts_to_analyze:\n",
    "#    print(text)\n",
    "len(texts_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        keywords = []\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            keywords.append(key)\n",
    "            if i > number:\n",
    "                break\n",
    "            \n",
    "        return keywords\n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elon',\n",
       " 'people',\n",
       " 'dogecoin',\n",
       " 'edda',\n",
       " 'coin',\n",
       " 'buy',\n",
       " 'doge',\n",
       " 'money',\n",
       " 'need',\n",
       " 'sell',\n",
       " 'thanks',\n",
       " 'btc',\n",
       " 'bitcoin',\n",
       " 'whales',\n",
       " 'tesla',\n",
       " 'crypto',\n",
       " 'believe',\n",
       " 'want',\n",
       " 'know',\n",
       " 'help',\n",
       " 'coins',\n",
       " 'support']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze('\\n'.join(texts_to_analyze), window_size=3, candidate_pos=['NOUN', 'VERB'],  lower=True)\n",
    "keywords = tr4w.get_keywords(20)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hub.load(\"./universal-sentence-encoder-multilingual-large_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model(texts_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_tensor = torch.Tensor(embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_trans = torch.transpose(embeddings_tensor, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.matmul(embeddings_tensor, embeddings_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_scores = torch.where(scores > 0.6, 1 , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_scores = torch.triu(cond_scores, diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(len(texts_to_analyze)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = torch.nonzero(graph_scores).numpy()\n",
    "G.add_edges_from(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(nx.algorithms.community.label_propagation.asyn_lpa_communities(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = {}\n",
    "clusters_examples = {}\n",
    "for i in range(len(c)):\n",
    "    sentence_ids = c[i]\n",
    "    sentences = []\n",
    "    for sid in sentence_ids:\n",
    "        s = texts_to_analyze[sid]\n",
    "        sentences.append(s)\n",
    "    centroid = np.mean(model(sentences).numpy(), 0)\n",
    "    centroids[i] = centroid\n",
    "    clusters_examples[i] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassign the sentences\n",
    "def merge_classes(centroids, clusters_examples):\n",
    "    def calculate_similarity(matrix, vector):\n",
    "        \"\"\"\n",
    "        Compute the cosine distances between each row of matrix and vector.\n",
    "        \"\"\"\n",
    "        v = vector.reshape(1, -1)\n",
    "        return 1 - spatial.distance.cdist(matrix, v, 'cosine').reshape(-1)\n",
    "\n",
    "    deleted = set()\n",
    "    new_clusters_examples = copy.deepcopy(clusters_examples)\n",
    "    new_centroids = copy.deepcopy(centroids)\n",
    "    centroid_ids = list(centroids.keys())\n",
    "    centroid_ids_len = len(centroid_ids) \n",
    "    for idx1 in range(centroid_ids_len - 1):\n",
    "        for idx2 in range(idx1+1, centroid_ids_len):\n",
    "            center_id1 = centroid_ids[idx1]\n",
    "            center_id2 = centroid_ids[idx2]\n",
    "            centre1 = centroids[center_id1]\n",
    "            centre2 = centroids[center_id2]\n",
    "            scores = calculate_similarity([centre1], centre2)\n",
    "            score = scores[0]\n",
    "            if score >= 0.6:\n",
    "                #print(\" merging \", center_id1, \" and \", center_id2)\n",
    "                id_to_delete = max(center_id1, center_id2)\n",
    "                id_to_keep = min(center_id1, center_id2)\n",
    "                deleted.add(id_to_delete)\n",
    "                new_clusters_examples[id_to_keep].extend(clusters_examples[id_to_delete])\n",
    "\n",
    "    for d in deleted:\n",
    "        del new_centroids[d] \n",
    "        del new_clusters_examples[d] \n",
    "    clusters_examples = new_clusters_examples\n",
    "    centroids = new_centroids\n",
    "    \n",
    "    return centroids, clusters_examples\n",
    "\n",
    "centroids, clusters_examples = merge_classes(centroids, clusters_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open(\"output.txt\", \"w\") as f:\n",
    "    for c in clusters_examples:\n",
    "        sentences = clusters_examples[c]\n",
    "        if len(sentences) > 3:\n",
    "            tr4w.analyze('\\n'.join(sentences), window_size=3, candidate_pos=['NOUN', 'VERB'],  lower=True)\n",
    "            keywords = tr4w.get_keywords(1)\n",
    "\n",
    "            f.write(\" \".join(keywords) + \"\\n\")\n",
    "\n",
    "            f.write(\"----------------\\n\")\n",
    "            for s in sentences:\n",
    "                f.write(s + \"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gold_mining",
   "language": "python",
   "name": "gold_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
